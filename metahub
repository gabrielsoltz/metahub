#!/usr/bin/env python3

import json
from sys import argv, exit

from helpers import (color, get_logger, get_parser, print_banner, print_table,
                     print_title_line, confirm_choice, test_python_version)
from AwsHelpers import get_account_id, get_account_alias
from securityhub import SecurityHub

from alive_progress import alive_bar

def generate_findings(
    logger, sh_filters, metachecks, mh_filters_checks, metatags, mh_filters_tags, sh_account, sh_role, mh_role
):
    mh_findings = {}
    mh_findings_not_matched_findings = {}
    mh_findings_short = {}
    mh_inventory = []
    mh_statistics = {
        "Title": {},
        "SeverityLabel": {},
        "Workflow": {},
        "RecordState": {},
        "Compliance": {},
        "ProductArn": {},
        "ResourceType": {},
        "AwsAccountId": {},
        "ResourceId": {}
    }
    sh = SecurityHub(logger, sh_account, sh_role)
    sh_findings = sh.get_findings(sh_filters)
    print_title_line("")
    print_table("Security Hub findings found: ", len(sh_findings))
    with alive_bar(
        total=len(sh_findings)
    ) as bar:
        for finding in sh_findings:
            bar.title = f"-> Analyzing findings..."

            mh_matched = False
            resource_arn, finding_parsed = sh.parse_finding(finding)

            # MetaChecks and MetaTags:
            if metachecks or metatags:
                from metachecks.metachecks import run_metachecks
                from metatags.metatags import run_metatags

                # If the resource was already matched, we don't run metachecks or metatags again but we show others findings
                if resource_arn in mh_findings:
                    mh_matched = True
                elif resource_arn in mh_findings_not_matched_findings:
                    mh_matched = False
                else:
                    if metachecks:
                        # We run metachecks only once, if the resource is in mh_findings or mh_findings_not_matched_findings, means it was already evaluated:
                        if (
                            not resource_arn in mh_findings
                            and not resource_arn in mh_findings_not_matched_findings
                        ):
                            mh_values, mh_checks_matched = run_metachecks(
                                logger, finding, mh_filters_checks, mh_role
                            )
                    else:
                        mh_checks_matched = True
                    if metatags:
                        # We run metatags only once:
                        if (
                            not resource_arn in mh_findings
                            and not resource_arn in mh_findings_not_matched_findings
                        ):
                            mh_tags, mh_tags_matched = run_metatags(
                                logger, finding, mh_filters_tags, mh_role
                            )
                    else:
                        mh_tags_matched = True
                    # If both checks are True we show the resource
                    if mh_tags_matched and mh_checks_matched:
                        mh_matched = True
            else:
                # If no metachecks and no metatags, we enforce to True the match so we show the resource:
                mh_matched = True

            # We keep a dict with no matched resources so we don't run MetaChecks again
            if not mh_matched:
                # We add the resource in our output only once:
                if not resource_arn in mh_findings_not_matched_findings:
                    mh_findings_not_matched_findings[resource_arn] = {}

            # We show the resouce only if matched MetaChecks (or Metachecks are disabled)
            if mh_matched:

                # STATISTICS (we count by matched finding)

                # Resource ARN
                if not resource_arn in mh_statistics["ResourceId"]:
                    mh_statistics["ResourceId"][resource_arn] = 0
                mh_statistics["ResourceId"][resource_arn] += 1

                for key, value in finding_parsed.items():
                    # Title
                    if not key in mh_statistics["Title"]:
                        mh_statistics["Title"][key] = 0
                    mh_statistics["Title"][key] += 1
                    # Values
                    for v in value:
                        if v == "Workflow" or v == "Compliance":
                            if value[v]:
                                if not value[v]["Status"] in mh_statistics[v]:
                                    mh_statistics[v][value[v]["Status"]] = 0
                                mh_statistics[v][value[v]["Status"]] += 1
                                continue
                        if v == "Id":
                            continue
                        if not value[v] in mh_statistics[v]:
                            mh_statistics[v][value[v]] = 0
                        mh_statistics[v][value[v]] += 1
                    # AwsAccountId
                    if not finding["AwsAccountId"] in mh_statistics["AwsAccountId"]:
                        mh_statistics["AwsAccountId"][finding["AwsAccountId"]] = 0
                    mh_statistics["AwsAccountId"][finding["AwsAccountId"]] += 1
                    # ResourceType
                    if not finding["Resources"][0]["Type"] in mh_statistics["ResourceType"]:
                        mh_statistics["ResourceType"][finding["Resources"][0]["Type"]] = 0
                    mh_statistics["ResourceType"][finding["Resources"][0]["Type"]] += 1

                # RESOURCE (we add the resource only once)
                if not resource_arn in mh_findings:
                    # Short
                    mh_findings[resource_arn] = {"findings": []}
                    mh_findings[resource_arn]["AwsAccountId"] = finding["AwsAccountId"]
                    mh_findings[resource_arn]["ResourceType"] = finding["Resources"][0]["Type"]
                    # Standard
                    mh_findings_short[resource_arn] = {"findings": []}
                    mh_findings_short[resource_arn]["AwsAccountId"] = finding["AwsAccountId"]
                    mh_findings_short[resource_arn]["ResourceType"] = finding["Resources"][0]["Type"]

                    # INVENTORY
                    mh_inventory.append(resource_arn)

                    # METACHECKS:
                    if metachecks:
                        # Short
                        mh_findings_short[resource_arn]["metachecks"] = mh_values
                        # Standard
                        mh_findings[resource_arn]["metachecks"] = mh_values

                    
                    # METATAGS
                    if metatags:
                        # Short
                        mh_findings_short[resource_arn]['metatags'] = mh_tags
                        # Standard
                        mh_findings[resource_arn]['metatags'] = mh_tags

                # FINDINGS
                mh_findings_short[resource_arn]["findings"].append(
                    list(finding_parsed.keys())[0]
                )
                mh_findings[resource_arn]["findings"].append(finding_parsed)
            bar()
        bar.title = f"-> Completed"
    
    #Sort Statistics
    for key_to_sort in mh_statistics:
        mh_statistics[key_to_sort] = dict(sorted(mh_statistics[key_to_sort].items(), key=lambda item: item[1], reverse=True))

    return mh_findings, mh_findings_short, sh_findings, mh_inventory, mh_statistics


def update_findings(logger, mh_findings, update, sh_account, sh_role):
    UpdateFilters = {}
    IsNnoteProvided = False
    IsAllowedKeyProvided = False
    for key, value in update.items():
        if key in ("Workflow", "Note"):
            if key == "Workflow":
                WorkflowValues = ("NEW", "NOTIFIED", "RESOLVED", "SUPPRESSED")
                if value not in WorkflowValues:
                    logger.error("Workflow values: " + str(WorkflowValues))
                    exit(1)
                Workflow = {"Workflow": {"Status": value}}
                UpdateFilters.update(Workflow)
                IsAllowedKeyProvided = True
            if key == "Note":
                Note = {"Note": {"Text": value, "UpdatedBy": "MetaHub"}}
                UpdateFilters.update(Note)
                IsNnoteProvided = True
            continue
        logger.error(
            "Unsuported update finding key: " + str(key) + " - Supported Keys: Workflow"
        )
        exit(1)
    if not IsAllowedKeyProvided:
        logger.error(
            'Missing Key to Update in update findings command. Please add Key="This is a value"'
        )
        exit(1)
    if not IsNnoteProvided:
        logger.error(
            'Missing Note in update findings command. Please add Note="This is an example Note"'
        )
        exit(1)
    # Run Update
    sh = SecurityHub(logger, sh_account, sh_role)
    if confirm_choice("Are you sure you want to update all findings?"):
        update_multiple = sh.update_findings_workflow(mh_findings, UpdateFilters)
        update_multiple_ProcessedFinding = []
        update_multiple_UnprocessedFindings = []
        for update in update_multiple:
            for ProcessedFinding in update["ProcessedFindings"]:
                logger.info("Updated Finding : " + ProcessedFinding["Id"])
                update_multiple_ProcessedFinding.append(ProcessedFinding)
            for UnprocessedFinding in update["UnprocessedFindings"]:
                logger.error(
                    "Error Updating Finding: "
                    + UnprocessedFinding["FindingIdentifier"]["Id"]
                    + " Error: "
                    + UnprocessedFinding["ErrorMessage"]
                )
                update_multiple_UnprocessedFindings.append(UnprocessedFinding)
        return update_multiple_ProcessedFinding, update_multiple_UnprocessedFindings
    return [], []

def enrich_findings(logger, mh_findings, sh_account, sh_role):
    sh = SecurityHub(logger, sh_account, sh_role)
    if confirm_choice("Are you sure you want to enrich all findings?"):
        update = sh.update_findings_meta(mh_findings)
        for ProcessedFinding in update["ProcessedFindings"]:
            logger.info("Updated Finding : " + ProcessedFinding["Id"])
        for UnprocessedFinding in update["UnprocessedFindings"]:
            logger.error(
                "Error Updating Finding: "
                + UnprocessedFinding["FindingIdentifier"]["Id"]
                + " Error: "
                + UnprocessedFinding["ErrorMessage"]
            )
    return update["ProcessedFindings"], update["UnprocessedFindings"]

def count_mh_findings(mh_findings):
    count = 0
    for resource in mh_findings:
        count += len(mh_findings[resource]["findings"])
    return count

def main(args):
    print_banner()
    test_python_version()
    parser = get_parser()
    args = parser.parse_args(args)
    logger = get_logger(args.log_level)

    def set_sh_filters(sh_filters):
        """Return filters for AWS Security Hub get_findings Call"""
        filters = {}
        for key, values in sh_filters.items():
            if key != "self" and values is not None:
                filters[key] = []
                for value in values:
                    value_to_append = {"Comparison": "EQUALS", "Value": value}
                    filters[key].append(value_to_append)
        return filters

    default_sh_filters = {
        "RecordState": ["ACTIVE"],
        "WorkflowStatus": ["NEW"],
        "ProductName": ["Security Hub"],
    }
    if not args.sh_filters and not args.sh_template:
        sh_filters = default_sh_filters
        sh_filters = set_sh_filters(sh_filters)
    elif args.sh_template:
        import yaml
        from pathlib import Path
        try:
            yaml_to_dict = yaml.safe_load(Path(args.sh_template).read_text())
            dict_values = next(iter(yaml_to_dict.values()))
            sh_filters = dict_values
        except yaml.scanner.ScannerError as err:
            logger.error("SH Template reading error: %s", err)
            exit (0)
        except FileNotFoundError:
            logger.error("SH Template file not found: %s", args.sh_template)
            exit (0)
    else:
        sh_filters = args.sh_filters
        sh_filters = set_sh_filters(sh_filters)

    default_mh_filters_checks = {}
    if not args.mh_filters_checks:
        mh_filters_checks = default_mh_filters_checks
    else:
        mh_filters_checks = args.mh_filters_checks
    for mh_filter_check_key, mh_filter_check_value in mh_filters_checks.items():
        if mh_filters_checks[mh_filter_check_key].lower() == "true":
            mh_filters_checks[mh_filter_check_key] = bool(True)
        elif mh_filters_checks[mh_filter_check_key].lower() == "false":
            mh_filters_checks[mh_filter_check_key] = bool(False)
        else:
            logger.error("MetaHub Filter Only True/False Supported: " + str(mh_filters_checks))
            exit(1)

    default_mh_filters_tags = {}
    if not args.mh_filters_tags:
        mh_filters_tags = default_mh_filters_tags
    else:
        mh_filters_tags = args.mh_filters_tags
    
    if (args.sh_account and not args.sh_assume_role) or (not args.sh_account and args.sh_assume_role):
        logger.error("Parameter error: --sh-assume-role and sh-account must be provided together, but only 1 provided.")
        exit(1)

    if not args.sh_account:
        sh_account = get_account_id()
        sh_account_alias = get_account_alias(logger)
    else:
        sh_account = args.sh_account
        sh_account_alias = get_account_alias(logger, sh_account, args.sh_assume_role)

    print_title_line("")
    print_table("Security Hub filters: ", str(sh_filters))
    print_table("Security Hub yaml: ", str(args.sh_template))
    print_table("MetaChecks: ", str(args.meta_checks))
    print_table("MetaChecks Filters: ", str(mh_filters_checks))
    print_table("MetaTags: ", str(args.meta_tags))
    print_table("MetaTags Filters: ", str(mh_filters_tags))
    print_table("Security Hub Account: ", str(sh_account) + " (" + str(sh_account_alias) + ")")
    print_table("Security Hub Role: ", str(args.sh_assume_role))
    print_table("Child Account Role: ", str(args.mh_assume_role))
    print_table("Output: ", str(args.output))
    print_table("Log Level: ", str(args.log_level))
    print_table("Update Findings: ", str(args.update_findings))

    if args.list_meta_checks:
        from metachecks.metachecks import list_metachecks

        print_title_line("List MetaChecks")
        list_metachecks(logger)
        exit(0)

    # Generate Findings
    (
        mh_findings,
        mh_findings_short,
        sh_findings,
        mh_inventory,
        mh_statistics,
    ) = generate_findings(
        logger,
        sh_filters,
        metachecks=args.meta_checks,
        mh_filters_checks=mh_filters_checks,
        metatags=args.meta_tags,
        mh_filters_tags=mh_filters_tags,
        sh_account=args.sh_account,
        sh_role=args.sh_assume_role,
        mh_role=args.mh_assume_role,
    )

    if args.list_findings:
        if mh_findings:
            for out in args.output:
                print_title_line("List Findings: " + out)
                if out == "short":
                    print(json.dumps(mh_findings_short, indent=2))
                if out == "inventory":
                    print(json.dumps(mh_inventory, indent=2))
                if out == "statistics":
                    print(json.dumps(mh_statistics, indent=2))
                if out == "standard":
                    print(json.dumps(mh_findings, indent=2))

    if args.write_json:
        if mh_findings:
            for out in args.output:
                print_title_line("Write Json: " + out)
                if out == "short":
                    with open(out + ".json", "w", encoding="utf-8") as f:
                        json.dump(mh_findings_short, f, indent=2)
                if out == "inventory":
                    with open(out + ".json", "w", encoding="utf-8") as f:
                        json.dump(mh_inventory, f, indent=2)
                if out == "statistics":
                    with open(out + ".json", "w", encoding="utf-8") as f:
                        json.dump(mh_statistics, f, indent=2)
                if out == "standard":
                    with open(out + ".json", "w", encoding="utf-8") as f:
                        json.dump(mh_findings, f, indent=2)
                print_table("File: ", out + ".json")
    
    if args.write_html:
        from json2html import json2html

        if mh_findings:
            for out in args.output:
                print_title_line("Write HTML: " + out)
                if out == "short":
                    with open(out + ".html", "w", encoding="utf-8") as f:
                        html = json2html.convert(json = mh_findings_short)
                        f.write(html)
                if out == "inventory":
                    with open(out + ".html", "w", encoding="utf-8") as f:
                        html = json2html.convert(json = mh_inventory)
                        f.write(html)
                        print_table("File: ", out + ".html")
                if out == "statistics":
                    with open(out + ".html", "w", encoding="utf-8") as f:
                        html = json2html.convert(json = mh_statistics)
                        f.write(html)
                        print_table("File: ", out + ".html")
                if out == "standard":
                    with open(out + ".html", "w", encoding="utf-8") as f:
                        html = json2html.convert(json = mh_findings)
                        f.write(html)
                print_table("File: ", out + ".html")

    if args.write_csv:
        import csv

        def output_csv(output):
            metachecks_columns = args.write_csv_meta_checks_columns
            metatags_columns = args.write_csv_meta_tags_columns
            new_list = []
            for key, dictionary in output.items():
                new_dict = {'ARN': key}
                if metatags_columns:
                    for column in metatags_columns:
                        try:
                            dictionary[column] = dictionary['metatags'][column]
                        except KeyError:
                            dictionary[column] = ''
                if metachecks_columns:
                    for column in metachecks_columns:
                        try:
                            dictionary[column] = dictionary['metachecks'][column]
                        except KeyError:
                            dictionary[column] = ''
                new_dict.update(dictionary)
                new_list.append(new_dict)
            columns = new_list[0].keys()
            return columns, new_list

        if mh_findings:
            for out in args.output:
                print_title_line("Write CSV: " + out)
                if out == "short":
                    with open(out + ".csv", "w", encoding="utf-8", newline='') as output_file:
                        columns, csv_list = output_csv(mh_findings_short)
                        dict_writer = csv.DictWriter(output_file, columns)
                        dict_writer.writeheader()
                        dict_writer.writerows(csv_list)
                if out == "inventory":
                    with open(out + ".csv", "w", encoding="utf-8", newline='') as output_file:
                        columns = ["ARN"]
                        dict_writer = csv.writer(output_file)
                        dict_writer.writerow(columns)
                        dict_writer.writerows([mh_inventory])
                if out == "statistics":
                    logger.error("CSV Output for Statistics not implemented...")
                if out == "standard":
                    with open(out + ".csv", "w", encoding="utf-8", newline='') as output_file:
                        columns, csv_list = output_csv(mh_findings)
                        dict_writer = csv.DictWriter(output_file, columns)
                        dict_writer.writeheader()
                        dict_writer.writerows(csv_list)
                if out != "statistics":
                    print_table("File: ", out + ".csv")



    print_title_line("Results")
    print_table("Non-Compliant Resources: ", str(len(mh_findings)))
    print_table("Non-Compliant Findings: ", str(count_mh_findings(mh_findings)))

    if args.update_findings:
        print_title_line("Update Findings")
        print_table("Findings: ", str(count_mh_findings(mh_findings)))
        print_table("Update: ", str(args.update_findings))
        if not mh_findings:
            print(color["BOLD"] + "Nothing to update..." + color["END"])
            exit(1)
        ProcessedFindings, UnprocessedFindings = update_findings(
            logger,
            mh_findings,
            args.update_findings,
            args.sh_account,
            args.sh_assume_role,
        )
        print_title_line("Results")
        print_table("ProcessedFindings: ", str(len(ProcessedFindings)))
        print_table("UnprocessedFindings: ", str(len(UnprocessedFindings)))

    if args.enrich_findings:
        print_title_line("Enrich Findings")
        print_table("Findings: ", str(count_mh_findings(mh_findings)))
        if not mh_findings:
            print(color["BOLD"] + "Nothing to update..." + color["END"])
            exit(1)
        ProcessedFindings, UnprocessedFindings = enrich_findings(
            logger,
            mh_findings,
            args.sh_account,
            args.sh_assume_role,
        )
        print_title_line("Results")
        print_table("ProcessedFindings: ", str(len(ProcessedFindings)))
        print_table("UnprocessedFindings: ", str(len(UnprocessedFindings)))

if __name__ == "__main__":
    main(argv[1:])
